{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Task_3_Box_LOST.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### **Applying LOST to predict a bounding-box for the first video frames**\n",
        "\n",
        "For implementing LOST: https://arxiv.org/pdf/2109.14279.pdf, we adapted the code of the paper, which is given on the following GitHub page: https://github.com/valeoai/LOST.\n",
        "\n",
        "First we are going to apply LOST on all first frames of 13 videos of the Something-Something dataset and predict bounding-boxes. The 13 videos are given on: https://github.com/joaanna/something_else.\n",
        "\n"
      ],
      "metadata": {
        "id": "_qnOjSdyWRtM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/valeoai/LOST"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GCf670g_pvgA",
        "outputId": "89fec032-6ccc-41b2-91fa-7b5c2c8d0662"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'LOST'...\n",
            "remote: Enumerating objects: 83, done.\u001b[K\n",
            "remote: Counting objects: 100% (83/83), done.\u001b[K\n",
            "remote: Compressing objects: 100% (66/66), done.\u001b[K\n",
            "remote: Total 83 (delta 31), reused 57 (delta 14), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (83/83), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/facebookresearch/dino.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kR_L4grNt7dH",
        "outputId": "28891a93-09b6-429f-af41-b98887a6b356"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'dino'...\n",
            "remote: Enumerating objects: 168, done.\u001b[K\n",
            "remote: Counting objects: 100% (104/104), done.\u001b[K\n",
            "remote: Compressing objects: 100% (27/27), done.\u001b[K\n",
            "remote: Total 168 (delta 88), reused 77 (delta 77), pack-reused 64\u001b[K\n",
            "Receiving objects: 100% (168/168), 24.45 MiB | 33.20 MiB/s, done.\n",
            "Resolving deltas: 100% (104/104), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Access to dataset through Drive\n",
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/', force_remount=True)\n",
        "os.chdir('/content/drive/My Drive/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bNy6a1GK_2xY",
        "outputId": "b351e9e7-5100-4b46-b4d1-4ca6953d4fc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import torch"
      ],
      "metadata": {
        "id": "SMVrm78onSJD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sys.path.insert(0, '/content/LOST')\n",
        "sys.path.insert(1, '/content/dino')\n",
        "sys.path.insert(2, '/content/drive/MyDrive/STCN/LOST-bounding-box/JPEGImages')\n",
        "sys.path.insert(3, '/content/drive/MyDrive/STCN/LOST-bounding-box/Annotations')"
      ],
      "metadata": {
        "id": "Oyv7lPHi0CNg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from object_discovery import patch_scoring, detect_box\n",
        "from skimage.transform import resize\n",
        "import scipy"
      ],
      "metadata": {
        "id": "nfPlKcOSK6cH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model\n",
        "k_patches = 100    # Number of patches with the lowest degree considered\n",
        "patch_size = 16\n",
        "from networks import get_model\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "model = get_model(\"vit_base\", patch_size = patch_size, resnet_dilate=0, device=device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jfJlqQU60AZZ",
        "outputId": "988af372-a0bb-436d-d83d-41843b8048ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Since no pretrained weights have been provided, we load the reference pretrained DINO weights.\n",
            "Pretrained weights found at dino_vitbase16_pretrain/dino_vitbase16_pretrain.pth and loaded with msg: <All keys matched successfully>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Image transformation\n",
        "from torchvision import transforms as pth_transforms\n",
        "transform = pth_transforms.Compose(\n",
        "    [\n",
        "        pth_transforms.ToTensor(),\n",
        "        pth_transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "Uhj9Jyfo0GK9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LOST: bounding-box predictions\n",
        "def lost(feats, dims, scales, init_image_size, k_patches=100):\n",
        "    \"\"\"\n",
        "    Inputs\n",
        "        feats: the pixel/patche features of an image\n",
        "        dims: dimension of the map from which the features are used\n",
        "        scales: from image to map scale\n",
        "        init_image_size: size of the image\n",
        "        k_patches: number of k patches retrieved that are compared to the seed at seed expansion\n",
        "    Outputs\n",
        "        pred: box predictions\n",
        "        A: binary affinity matrix\n",
        "        scores: lowest degree scores for all patches\n",
        "        seed: selected patch corresponding to an object\n",
        "    \"\"\"\n",
        "    # Compute the similarity\n",
        "    A = (feats @ feats.transpose(1, 2)).squeeze()\n",
        "\n",
        "    # Compute the inverse degree centrality measure per patch\n",
        "    sorted_patches, scores = patch_scoring(A)\n",
        "\n",
        "    # Select the initial seed\n",
        "    seed = sorted_patches[0]\n",
        "\n",
        "    # Seed expansion\n",
        "    potentials = sorted_patches[:k_patches]\n",
        "    similars = potentials[A[seed, potentials] > 0.0]\n",
        "    M = torch.sum(A[similars, :], dim=0)\n",
        "\n",
        "    # Box extraction\n",
        "    pred, _ = detect_box(\n",
        "        M, seed, dims, scales=scales, initial_im_size=init_image_size[1:]\n",
        "    )\n",
        "\n",
        "    return np.asarray(pred), A, scores, seed"
      ],
      "metadata": {
        "id": "h9yeyrHaFRKZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Video folder names\n",
        "for folder in os.walk(sys.path[2], topdown=True):\n",
        "    video_names = folder[1]\n",
        "    break\n",
        "\n",
        "print('Video folder names: ', video_names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZCuLj7hS9_-E",
        "outputId": "f41c6e75-8cdb-4979-ca86-45d9f535c447"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Video folder names:  ['44862', '57082', '2', '151201', '4', '80962', '3201', '862', '77005', '13201', '2003', '6981', '22983']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for video_name in video_names:\n",
        "    image_path = os.path.join(sys.path[2], video_name, '0001.jpg')  # First frames in the folders\n",
        "    image = Image.open(image_path)\n",
        "    image = np.array(image)\n",
        "    image_size = image.shape                    # array (image_height, image_width, 3)\n",
        "\n",
        "    # Apply LOST\n",
        "    img = image\n",
        "    img = transform(img)\n",
        "    init_image_size = img.shape    # tensor [3, image_height, image_width]\n",
        "    im_name = '0001'\n",
        "\n",
        "    # Padding the image with zeros to fit multiple of patch-size\n",
        "    size_im = (\n",
        "        img.shape[0],\n",
        "        int(np.ceil(img.shape[1] / patch_size) * patch_size),\n",
        "        int(np.ceil(img.shape[2] / patch_size) * patch_size),\n",
        "    )\n",
        "    paded = torch.zeros(size_im)\n",
        "    paded[:, : img.shape[1], : img.shape[2]] = img\n",
        "    img = paded\n",
        "    # img = img.cuda(non_blocking=True)    # Move to gpu\n",
        "\n",
        "    # Size for transformers\n",
        "    w_featmap = img.shape[-2] // patch_size\n",
        "    h_featmap = img.shape[-1] // patch_size\n",
        "\n",
        "    which_features = \"k\"    # possible choices : \"q\", \"k\", \"v\"\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Store the outputs of qkv layer from the last attention layer\n",
        "        feat_out = {}\n",
        "        def hook_fn_forward_qkv(module, input, output):\n",
        "            feat_out[\"qkv\"] = output\n",
        "\n",
        "        model._modules[\"blocks\"][-1]._modules[\"attn\"]._modules[\"qkv\"].register_forward_hook(hook_fn_forward_qkv)\n",
        "\n",
        "        # Forward pass in the model\n",
        "        attentions = model.get_last_selfattention(img[None, :, :, :])\n",
        "\n",
        "        # Scaling factor\n",
        "        scales = [patch_size, patch_size]\n",
        "\n",
        "        # Dimensions\n",
        "        nb_im = attentions.shape[0]  # Batch size\n",
        "        nh = attentions.shape[1]  # Number of heads\n",
        "        nb_tokens = attentions.shape[2]  # Number of tokens\n",
        "\n",
        "        # Extract the qkv features of the last attention layer\n",
        "        qkv = (\n",
        "            feat_out[\"qkv\"]\n",
        "            .reshape(nb_im, nb_tokens, 3, nh, -1 // nh)\n",
        "            .permute(2, 0, 3, 1, 4)\n",
        "        )\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "        k = k.transpose(1, 2).reshape(nb_im, nb_tokens, -1)\n",
        "        q = q.transpose(1, 2).reshape(nb_im, nb_tokens, -1)\n",
        "        v = v.transpose(1, 2).reshape(nb_im, nb_tokens, -1)\n",
        "\n",
        "        # Modality selection\n",
        "        if which_features == \"k\":     # keys of the patches\n",
        "            feats = k[:, 1:, :]       # tensor [1, 1620, 768]\n",
        "        elif which_features == \"q\":\n",
        "            feats = q[:, 1:, :]\n",
        "        elif which_features == \"v\":\n",
        "            feats = v[:, 1:, :]\n",
        "\n",
        "    # Bounding-box prediction\n",
        "    pred, A, scores, seed = lost(\n",
        "        feats,\n",
        "        [w_featmap, h_featmap],\n",
        "        scales,\n",
        "        init_image_size,\n",
        "        k_patches=k_patches,\n",
        "    )\n",
        "\n",
        "    # Bounding-box coordinates\n",
        "    xmin, ymin, xmax, ymax = pred\n",
        "\n",
        "    # Create a red bounding box\n",
        "    mask = image * 0                         # array (image_height, image_width, 3)\n",
        "    mask[ymin:ymax, xmin:xmax, 0] = 255\n",
        "    mask = Image.fromarray(mask, 'RGB')\n",
        "\n",
        "    # Save mask\n",
        "    if not os.path.isdir(os.path.join(sys.path[3], video_name)):\n",
        "        os.makedirs(os.path.join(sys.path[3], video_name))\n",
        "    mask.save(os.path.join(sys.path[3], video_name, '0001.png'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M4Se5qcNoCb1",
        "outputId": "de719c07-b1e2-407a-a780-ae9abde267e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3635: UserWarning: Default upsampling behavior when mode=bicubic is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
            "  \"See the documentation of nn.Upsample for details.\".format(mode)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
            "  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "_nnPA7IfccGG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}