{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Task3_MaskRCNN_and_PointRend.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"e12621e6d3474d6b8c4f807fdae95e8a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_cca78c385a354ba9b8d02216f4cfd3a4","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_1c6c1d91c81a4d5390b4f4175904c6ae","IPY_MODEL_04f38c557454453488ba4fbe1d513bf5","IPY_MODEL_caf85a25f6be488e9cf24f2ee007f8a5"]}},"cca78c385a354ba9b8d02216f4cfd3a4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"1c6c1d91c81a4d5390b4f4175904c6ae":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_0ffea91e5eb64eee92a2cf5226c575f2","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_c7c3776536b44999bc192d8906902d8f"}},"04f38c557454453488ba4fbe1d513bf5":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_17bb42663ec24594bd4f1f3a1bb538c7","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":102530333,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":102530333,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_df68aeefbb8849e99d71e6e35bc3dde8"}},"caf85a25f6be488e9cf24f2ee007f8a5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_406899d2252240a6a1795df6f83ff87f","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 97.8M/97.8M [00:00&lt;00:00, 218MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_0dad2ea836834e15a421ddaa5e288ea3"}},"0ffea91e5eb64eee92a2cf5226c575f2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"c7c3776536b44999bc192d8906902d8f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"17bb42663ec24594bd4f1f3a1bb538c7":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"df68aeefbb8849e99d71e6e35bc3dde8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"406899d2252240a6a1795df6f83ff87f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"0dad2ea836834e15a421ddaa5e288ea3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"bcd4de69dd364013824e8e78e9288df3":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_5c9ab639ee3b4831842ebb756d3d95d4","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_61504f990e164eea95abe7d1b5a052d1","IPY_MODEL_e6acc1563332490ca84fb6e87ae1e36c","IPY_MODEL_11f287e6770f4e7889c0bf0b3e628c86"]}},"5c9ab639ee3b4831842ebb756d3d95d4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"61504f990e164eea95abe7d1b5a052d1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_7303a523cf6340e3896c0dff45b3d99a","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_cd91d0c5290b4a838c8540dae8f83820"}},"e6acc1563332490ca84fb6e87ae1e36c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_cdb60eddc13f4ec9bd5d97f762872e8f","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":46827520,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":46827520,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_89b9c9ea91e34a69bd9fb47dbe718ccd"}},"11f287e6770f4e7889c0bf0b3e628c86":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_eb69805cc0b04187b4e4c930a6e4be03","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 44.7M/44.7M [00:00&lt;00:00, 135MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_f89ea24c355546bf9ca639b179b9ea83"}},"7303a523cf6340e3896c0dff45b3d99a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"cd91d0c5290b4a838c8540dae8f83820":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"cdb60eddc13f4ec9bd5d97f762872e8f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"89b9c9ea91e34a69bd9fb47dbe718ccd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"eb69805cc0b04187b4e4c930a6e4be03":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"f89ea24c355546bf9ca639b179b9ea83":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"0d2d8b53943040ef9572917279eed444":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_2a09707921864db7857f7f4d014d5781","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_fcf6bb6cb4e744bd900219856ea44a37","IPY_MODEL_52ce95f48c994917a88c08ef4f3bdd5a","IPY_MODEL_5a65a018ebe34b6ebb67d6877d176088"]}},"2a09707921864db7857f7f4d014d5781":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"fcf6bb6cb4e744bd900219856ea44a37":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_f66d0bc200584cbfb70cd6f2bcae26bd","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_f0f18603696c4c79b19ee7047259e8d3"}},"52ce95f48c994917a88c08ef4f3bdd5a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_25f75c3048704b6f8da85b2ced8ee6d2","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":178090079,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":178090079,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_200047acc44a40ae9c50a9f2523fedf8"}},"5a65a018ebe34b6ebb67d6877d176088":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_8810aa4ce0ae4765b922f6922e8da345","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 170M/170M [00:01&lt;00:00, 113MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_bd68a01b53504784b36b07f6a675ce21"}},"f66d0bc200584cbfb70cd6f2bcae26bd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"f0f18603696c4c79b19ee7047259e8d3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"25f75c3048704b6f8da85b2ced8ee6d2":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"200047acc44a40ae9c50a9f2523fedf8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"8810aa4ce0ae4765b922f6922e8da345":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"bd68a01b53504784b36b07f6a675ce21":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","source":["# Access to dataset through Drive\n","import os\n","from google.colab import drive\n","drive.mount('/content/drive/', force_remount=True)\n","os.chdir('/content/drive/My Drive/')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DW86k7tgMnnT","outputId":"cea3a922-568c-437b-c692-b315d80f8030"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n"]}]},{"cell_type":"code","source":["%cd '/content/drive/MyDrive/STCN/STCN'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ItsS3eP5MyXZ","outputId":"10750087-715d-463b-c398-117bf51eed4e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/.shortcut-targets-by-id/1O_DxTckzGnTlprgYFmKup7Ct6cS1OJiW/STCN/STCN\n"]}]},{"cell_type":"code","source":["import os\n","from os import path\n","import time\n","from argparse import ArgumentParser\n","\n","import torch\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader\n","import numpy as np\n","from PIL import Image\n","\n","from model.eval_network import STCN\n","from dataset.davis_test_dataset import DAVISTestDataset\n","from util.tensor_util import unpad\n","from inference_core import InferenceCore\n","\n","from progressbar import progressbar\n","\n","import tqdm\n","\n","import glob\n","import json\n","\n","import cv2\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import matplotlib.patches as patches\n","from pycocotools.coco import COCO\n","\n","import torch.optim as optim\n","from torch import nn, Tensor\n","from torch.utils.data import Dataset\n","import torchvision\n","import torchvision.transforms.functional as TF\n","from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","import torchvision.models as models\n","\n","import math"],"metadata":{"id":"Lk_t01yW2XQI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Setuping STCN model"],"metadata":{"id":"zPqEzTLlMs9L"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"tVEuZxr1Zv_Y"},"outputs":[],"source":["# Authors' implementation of STCN on Davis2017\n","# https://github.com/hkchengrex/STCN/blob/main/eval_davis.py\n","\n","\"\"\"\n","Arguments loading\n","\"\"\"\n","parser = ArgumentParser()\n","parser.add_argument('--model', default='/content/drive/MyDrive/STCN/STCN/saves/stcn.pth')\n","parser.add_argument('--davis_path', default='/content/drive/MyDrive/STCN/DAVIS/2017')\n","parser.add_argument('--output', default='/content/drive/MyDrive/STCN/experiment/Davis2017/Yujin/val_toast_fps') #saving_masks path\n","parser.add_argument('--split', help='val/testdev', default='val')\n","parser.add_argument('--top', type=int, default=20)\n","parser.add_argument('--amp', action='store_true')\n","parser.add_argument('--mem_every', default=5, type=int)\n","parser.add_argument('--include_last', help='include last frame as temporary memory?', action='store_true')\n","parser.add_argument('--visualisation', default=True, type=bool) # Save the visualisation\n","\n","args, unknown = parser.parse_known_args()\n","\n","davis_path = args.davis_path\n","out_path = args.output\n","VIZ = args.visualisation\n","\n","# Simple setup\n","os.makedirs(out_path, exist_ok=True)\n","palette = Image.open(path.expanduser(davis_path + '/trainval/Annotations/480p/blackswan/00000.png')).getpalette()"]},{"cell_type":"markdown","source":["# Loading Dataset for evaluation + Loading pretrained model"],"metadata":{"id":"zgGDEGzBM3RB"}},{"cell_type":"code","source":["torch.autograd.set_grad_enabled(False)\n","\n","# Setup Dataset\n","if args.split == 'val':\n","    test_dataset = DAVISTestDataset(davis_path + '/trainval', imset='2017/val.txt')\n","    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=1)\n","elif args.split == 'testdev':\n","    test_dataset = DAVISTestDataset(davis_path + '/test-dev', imset='2017/test-dev.txt')\n","    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=1)\n","else:\n","    raise NotImplementedError\n","\n","\n","use_cuda = torch.cuda.is_available()\n","if use_cuda:\n","    print('Using GPU')\n","else:\n","    print('Using CPU')\n","\n","\n","# Load our checkpoint\n","top_k = args.top\n","prop_model = STCN().cuda().eval()\n","\n","# Performs input mapping such that stage 0 model can be loaded\n","prop_saved = torch.load(args.model)\n","for k in list(prop_saved.keys()):\n","    if k == 'value_encoder.conv1.weight':\n","        if prop_saved[k].shape[1] == 4:\n","            pads = torch.zeros((64,1,7,7), device=prop_saved[k].device)\n","            prop_saved[k] = torch.cat([prop_saved[k], pads], 1)\n","prop_model.load_state_dict(prop_saved)"],"metadata":{"id":"zKWVvDw77uQ0","colab":{"base_uri":"https://localhost:8080/","height":150,"referenced_widgets":["e12621e6d3474d6b8c4f807fdae95e8a","cca78c385a354ba9b8d02216f4cfd3a4","1c6c1d91c81a4d5390b4f4175904c6ae","04f38c557454453488ba4fbe1d513bf5","caf85a25f6be488e9cf24f2ee007f8a5","0ffea91e5eb64eee92a2cf5226c575f2","c7c3776536b44999bc192d8906902d8f","17bb42663ec24594bd4f1f3a1bb538c7","df68aeefbb8849e99d71e6e35bc3dde8","406899d2252240a6a1795df6f83ff87f","0dad2ea836834e15a421ddaa5e288ea3","bcd4de69dd364013824e8e78e9288df3","5c9ab639ee3b4831842ebb756d3d95d4","61504f990e164eea95abe7d1b5a052d1","e6acc1563332490ca84fb6e87ae1e36c","11f287e6770f4e7889c0bf0b3e628c86","7303a523cf6340e3896c0dff45b3d99a","cd91d0c5290b4a838c8540dae8f83820","cdb60eddc13f4ec9bd5d97f762872e8f","89b9c9ea91e34a69bd9fb47dbe718ccd","eb69805cc0b04187b4e4c930a6e4be03","f89ea24c355546bf9ca639b179b9ea83"]},"outputId":"88f955e6-1d90-45bf-a122-458bac9af311"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Using GPU\n"]},{"output_type":"stream","name":"stderr","text":["Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e12621e6d3474d6b8c4f807fdae95e8a","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0.00/97.8M [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Downloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-5c106cde.pth\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"bcd4de69dd364013824e8e78e9288df3","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0.00/44.7M [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{},"execution_count":5}]},{"cell_type":"markdown","source":["# Utils functions\n","\n","Imshow dunction were mainly used to debug the process, it displays an image based on the tensor you give as argument. color and grayscale\n","\n","Finding bounding box function help to filter the segmentation model output in order to be able to measure correctly between authors' result and ours"],"metadata":{"id":"Jvh6ylynlZPr"}},{"cell_type":"code","source":["def imshow(img,title):\n","    #img = img * 0.5 + 0.2  # unnormalize\n","    npimg = img.detach().numpy()\n","    npimg = np.transpose(npimg,(1,2,0))\n","\n","    plt.figure(figsize=(16,16))\n","    plt.imshow(npimg)\n","    plt.title(title)\n","    plt.show()\n","\n","def imshow_bw(img,title):\n","    #img = img * 0.5 + 0.2  # unnormalize\n","    npimg = img.detach().numpy()\n","    #npimg = np.transpose(npimg,(1,2,0))\n","\n","    plt.figure(figsize=(10,10))\n","    plt.imshow(npimg,cmap='gray')\n","    plt.title(title)\n","    plt.show()\n","\n","def finding_bounding_box(mask):\n","  D,L,W = mask.shape\n","  bounding_box = torch.empty((D,4), dtype=torch.float)\n","  for d in range(D):\n","    flag_i =0\n","    flag_j = 0\n","    for i in range(L):\n","      if (mask[d,i,:].sum()>0.0 and flag_i == 0):\n","        y0 = float(i)\n","        flag_i = 1\n","      if ((mask[d,i,:].sum()==0.0 and flag_i == 1 )or i==L-1 and flag_i == 1):\n","        y1 = float(i)\n","        flag_i = 2\n","    for j in range(W):\n","      if (mask[d,:,j].sum()>0.0 and flag_j == 0):\n","        x0 = float(j)\n","        flag_j = 1\n","      if ((mask[d,:,j].sum() == 0.0 and flag_j == 1) or (j== W-1 and flag_j == 1)):\n","        x1=float(j)\n","        flag_j = 2\n","    bounding_box[d] = torch.tensor([[x0,y0,x1,y1]])\n","  return bounding_box\n","\n","def imshow_bw_box(img,box,title):\n","    #img = img * 0.5 + 0.2  # unnormalize\n","    npimg = img.detach().numpy()\n","    #npimg = np.transpose(npimg,(1,2,0))\n","    print(box)\n","    plt.figure(figsize=(16,16))\n","    plt.imshow(npimg,cmap='gray')\n","    ax = plt.gca()\n","    rect = patches.Rectangle((box[0],box[1]),box[2]-box[0],box[3]-box[1],linewidth=1, edgecolor='r', facecolor='none')\n","    ax.add_patch(rect)\n","    plt.title(title)\n","    plt.show()"],"metadata":{"id":"_BPZH45Edv7M"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#function predicting_mask for MaskRCNN and PointRend\n","\n","The two pretrained models have different output format, some small differences are in the two functions but the idea is the same.\n","\n","* As input of the function we have :\n","\n","1.   Output of segmentation model\n","2.   Bounding box of ground truth object for first frame\n","3.   some args\n","\n","We filter the output of the segmentation results by calculating the $L_2$ distance between the bounding box of ground truth objects and the one found by the segmentation models. The set of first frame masks is then computed."],"metadata":{"id":"tpKWOayrmSsC"}},{"cell_type":"code","source":["def predicting_mask_sync(prediction,bb_dataset,acceptance_rate=0.8,threshold_object = 0.5,display=False):\n","  '''\n","  input :\n","  -- prediction : output of Mask-R-CNN\n","  -- bb_dataset : mask of original GT objects to find the same with Mask-R-CNN\n","  -- acceptance_rate (default 0.8) : confidence level to allow an object to be found\n","  -- threshold_object (default 0.5) : threshold value for binary mask\n","  -- display (default False): option to display mask images + overall mask \n","  output :\n","  -- Give back pack of masks noramlized for STCN training\n","  -- Number of mask/objects discovered\n","  '''\n","  scores = prediction[0][\"scores\"]\n","  mask = prediction[0][\"masks\"]\n","  boxes = prediction[0][\"boxes\"]\n","\n","  obj_accepted = scores[scores>=acceptance_rate]\n","  obj_accepted = obj_accepted.shape\n","  INF = 9999999\n","\n","  D,_ = bb_dataset.shape #D = number of object in GT\n","\n","  idx_best_mask = np.zeros(D)\n","  #compares predicted boxes with GT boxes only X object > accpetance rate\n","  for objects in range(D):\n","    dist_saved = INF\n","    for predicted_boxes in range(obj_accepted[0]):\n","      dist = torch.cdist(torch.unsqueeze(bb_dataset[objects],0),torch.unsqueeze(boxes[predicted_boxes],0),2)\n","      if(dist < dist_saved):\n","        dist_saved = dist\n","        idx_best_mask[objects] = predicted_boxes\n","    \n","  shape_mask = mask[0,:,:,:].shape #I get the shape of the image\n","\n","  final_mask = torch.zeros(D,1,shape_mask[1],shape_mask[2]).cuda()\n","\n","  for i in range(D):\n","    final_mask[i,0,:,:]= mask[int(idx_best_mask[i]),0,:,:]\n","\n","  final_mask[final_mask>=threshold_object]=1.\n","  final_mask[final_mask<threshold_object]=0.\n","  return final_mask\n","\n","\n","def predicting_mask_sync_detectron2(prediction,bb_dataset,acceptance_rate=0.8,threshold_object = 0.5,display=False):\n","  '''\n","  input :\n","  -- prediction : output of Detectron2\n","  -- bb_dataset : mask of original GT objects to find the same with Mask-R-CNN\n","  -- acceptance_rate (default 0.8) : confidence level to allow an object to be found\n","  -- threshold_object (default 0.5) : threshold value for binary mask\n","  -- display (default False): option to display mask images + overall mask \n","  output :\n","  -- Give back pack of masks noramlized for STCN training\n","  -- Number of mask/objects discovered\n","  '''\n","  nbr_objects = len(prediction[\"instances\"])\n","  mask = prediction[\"instances\"][:].pred_masks\n","  scores = prediction[\"instances\"][:].scores\n","  boxes = prediction[\"instances\"][0:nbr_objects].pred_boxes\n","\n","\n","  obj_accepted = scores[scores>=0.01]\n","  obj_accepted = obj_accepted.shape\n","\n","  D,_ = bb_dataset.shape #D = number of object in GT\n","  INF = 9999999\n","\n","  idx_best_mask = np.zeros(D)\n","  #compares predicted boxes with GT boxes only X object > accpetance rate\n","  for objects in range(D):\n","    dist_saved = INF\n","    for i,predicted_boxes in enumerate(boxes):\n","      dist = torch.cdist(torch.unsqueeze(bb_dataset[objects],0),torch.unsqueeze(predicted_boxes,0),2)\n","      if(dist < dist_saved):\n","        dist_saved = dist\n","        idx_best_mask[objects] = i\n","\n","  shape_mask = mask.shape #I get the shape of the image\n","  final_mask = torch.zeros(D,1,shape_mask[1],shape_mask[2]).cuda()\n","\n","  for i in range(D):\n","      final_mask[i,0,:,:]= mask[int(idx_best_mask[i])]\n","\n","  final_mask[final_mask==True]=1.\n","  final_mask[final_mask==False]=0.\n","\n","\n","  return final_mask"],"metadata":{"id":"6Wrv5k_RhAou"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Preprocessing functions"],"metadata":{"id":"zql-vsrwIXGi"}},{"cell_type":"code","source":["def preprocessing_maskrcnn(rgb,std,mean):\n","    first_frame = rgb[0,0] #to get first image of video\n","    first_frame = first_frame * std + mean\n","    first_frame = torch.unsqueeze(first_frame, dim=0)\n","    first_frame = first_frame.cuda()\n","    return first_frame\n","\n","\n","def preprocessing_pointrend(rgb,std,mean):\n","    first_frame = rgb[0,0] #to get first image of video\n","    first_frame = first_frame * std + mean\n","    first_frame = (first_frame * 255)\n","    first_frame = torch.permute(first_frame,(1,2,0))\n","    first_frame_int = first_frame.float().numpy()\n","    return first_frame_int"],"metadata":{"id":"pjwHwCSdIViU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Algorithm segmentation"],"metadata":{"id":"k7mIfYM-Ii-e"}},{"cell_type":"code","source":["\n","def algo_maskrcnn(model,gt_mask,rgb,std,mean):\n","\n","    first_frame = preprocessing_maskrcnn(rgb,std,mean) #preprocessing first frame of video\n","    predict_mask = model(first_frame) #predicting objects segmentation\n","    bb_first_frame = finding_bounding_box(msk[:,0,0]).cuda() #used to filter segmentation model result\n","    msk_p = predicting_mask_sync(predict_mask,bb_first_frame,acceptance_rate=0.01) #generate first frame bounding box\n","    msk_p.cuda()\n","    return msk_p\n","    \n","def algo_pointrend(model,gt_mask,rgb,std,mean):\n","\n","    first_frame = preprocessing_pointrend(rgb,std,mean) #preprocessing first frame of video\n","    predict_mask = model(first_frame) #predicting objects segmentation\n","    bb_first_frame = finding_bounding_box(msk[:,0,0]).cuda() #used to filter segmentation model result\n","    msk_p = predicting_mask_sync_detectron2(predict_mask,bb_first_frame,acceptance_rate=0.01) #generate first frame bounding box\n","    msk_p.cuda()\n","    return msk_p"],"metadata":{"id":"NEyJDPmwIikJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Evaluation on STCN model using pretrained Mask-R-CNN as 1st frame segmentation"],"metadata":{"id":"rQMHsS-tXOsf"}},{"cell_type":"markdown","source":["## Calling pretrained models"],"metadata":{"id":"y-A6ITz2nHCh"}},{"cell_type":"code","source":["maskrcnn_resnet50_fpn = models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n","maskrcnn_resnet50_fpn.to(device).eval()"],"metadata":{"id":"PT0MrMjkis5W","colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["0d2d8b53943040ef9572917279eed444","2a09707921864db7857f7f4d014d5781","fcf6bb6cb4e744bd900219856ea44a37","52ce95f48c994917a88c08ef4f3bdd5a","5a65a018ebe34b6ebb67d6877d176088","f66d0bc200584cbfb70cd6f2bcae26bd","f0f18603696c4c79b19ee7047259e8d3","25f75c3048704b6f8da85b2ced8ee6d2","200047acc44a40ae9c50a9f2523fedf8","8810aa4ce0ae4765b922f6922e8da345","bd68a01b53504784b36b07f6a675ce21"]},"outputId":"5fc901cd-d5fb-46c3-e9d4-5d8b86901c86"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading: \"https://download.pytorch.org/models/maskrcnn_resnet50_fpn_coco-bf2d0c1e.pth\" to /root/.cache/torch/hub/checkpoints/maskrcnn_resnet50_fpn_coco-bf2d0c1e.pth\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0d2d8b53943040ef9572917279eed444","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0.00/170M [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["MaskRCNN(\n","  (transform): GeneralizedRCNNTransform(\n","      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n","  )\n","  (backbone): BackboneWithFPN(\n","    (body): IntermediateLayerGetter(\n","      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","      (bn1): FrozenBatchNorm2d(64, eps=0.0)\n","      (relu): ReLU(inplace=True)\n","      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n","      (layer1): Sequential(\n","        (0): Bottleneck(\n","          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n","          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n","          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","          (downsample): Sequential(\n","            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","            (1): FrozenBatchNorm2d(256, eps=0.0)\n","          )\n","        )\n","        (1): Bottleneck(\n","          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n","          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n","          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (2): Bottleneck(\n","          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n","          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n","          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","      )\n","      (layer2): Sequential(\n","        (0): Bottleneck(\n","          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n","          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n","          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","          (downsample): Sequential(\n","            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","            (1): FrozenBatchNorm2d(512, eps=0.0)\n","          )\n","        )\n","        (1): Bottleneck(\n","          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n","          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n","          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (2): Bottleneck(\n","          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n","          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n","          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (3): Bottleneck(\n","          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n","          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n","          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","      )\n","      (layer3): Sequential(\n","        (0): Bottleneck(\n","          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","          (downsample): Sequential(\n","            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","            (1): FrozenBatchNorm2d(1024, eps=0.0)\n","          )\n","        )\n","        (1): Bottleneck(\n","          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (2): Bottleneck(\n","          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (3): Bottleneck(\n","          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (4): Bottleneck(\n","          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (5): Bottleneck(\n","          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","      )\n","      (layer4): Sequential(\n","        (0): Bottleneck(\n","          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n","          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n","          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","          (downsample): Sequential(\n","            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","            (1): FrozenBatchNorm2d(2048, eps=0.0)\n","          )\n","        )\n","        (1): Bottleneck(\n","          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n","          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n","          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (2): Bottleneck(\n","          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n","          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n","          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","      )\n","    )\n","    (fpn): FeaturePyramidNetwork(\n","      (inner_blocks): ModuleList(\n","        (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n","        (1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n","        (2): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n","        (3): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n","      )\n","      (layer_blocks): ModuleList(\n","        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      )\n","      (extra_blocks): LastLevelMaxPool()\n","    )\n","  )\n","  (rpn): RegionProposalNetwork(\n","    (anchor_generator): AnchorGenerator()\n","    (head): RPNHead(\n","      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n","      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n","    )\n","  )\n","  (roi_heads): RoIHeads(\n","    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n","    (box_head): TwoMLPHead(\n","      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n","      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n","    )\n","    (box_predictor): FastRCNNPredictor(\n","      (cls_score): Linear(in_features=1024, out_features=91, bias=True)\n","      (bbox_pred): Linear(in_features=1024, out_features=364, bias=True)\n","    )\n","    (mask_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(14, 14), sampling_ratio=2)\n","    (mask_head): MaskRCNNHeads(\n","      (mask_fcn1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (relu1): ReLU(inplace=True)\n","      (mask_fcn2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (relu2): ReLU(inplace=True)\n","      (mask_fcn3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (relu3): ReLU(inplace=True)\n","      (mask_fcn4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (relu4): ReLU(inplace=True)\n","    )\n","    (mask_predictor): MaskRCNNPredictor(\n","      (conv5_mask): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n","      (relu): ReLU(inplace=True)\n","      (mask_fcn_logits): Conv2d(256, 91, kernel_size=(1, 1), stride=(1, 1))\n","    )\n","  )\n",")"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["### Hyper parameters ###\n","\n","\n","VIZ = False ## Option to generate video with mask (add more processing time)\n","\n","mean = torch.tensor([0.485, 0.456, 0.406])[:, None, None].cuda()\n","std = torch.tensor([0.229, 0.224, 0.225])[:, None, None].cuda()\n","\n","output_path = '/content/drive/MyDrive/STCN/experiment/Davis2017/Yujin/val_22jan/'"],"metadata":{"id":"AoCr4waNJXd8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Start evaluation ###\n","total_process_time = 0\n","total_frames = 0\n","\n","for data in test_loader:\n","    with torch.cuda.amp.autocast(enabled=args.amp):\n","        rgb = data['rgb'].cuda()\n","        msk = data['gt'][0].cuda() # original annotation\n","        info = data['info']\n","        name = info['name'][0]\n","        k = len(info['labels'][0])\n","        size = info['size_480p']\n","\n","        #############################################################################\n","        ######################### SEGMENTATION ALGORTIMH ############################\n","        #############################################################################\n","        msk_p = algo_maskrcnn(maskrcnn_resnet50_fpn,msk,rgb,std,mean)\n","        #############################################################################\n","\n","        torch.cuda.synchronize()\n","        process_begin = time.time()\n","\n","        processor = InferenceCore(prop_model, rgb, k, top_k=top_k, \n","                        mem_every=args.mem_every, include_last=args.include_last)\n","        processor.interact(msk_p, 0, rgb.shape[1]) #msk_p changed here\n","\n","        # Do unpad -> upsample to original size \n","        out_masks = torch.zeros((processor.t, 1, *size), dtype=torch.uint8, device='cuda')\n","        for ti in range(processor.t):\n","            prob = unpad(processor.prob[:,ti], processor.pad)\n","            prob = F.interpolate(prob, size, mode='bilinear', align_corners=False)\n","            out_masks[ti] = torch.argmax(prob, dim=0)\n","        \n","        out_masks = (out_masks.detach().cpu().numpy()[:,0]).astype(np.uint8)\n","\n","        torch.cuda.synchronize()\n","        total_process_time += time.time() - process_begin\n","        total_frames += out_masks.shape[0]\n","\n","        # Save the results\n","        this_out_path = path.join(out_path, name)\n","        os.makedirs(this_out_path, exist_ok=True)\n","        for f in range(out_masks.shape[0]):\n","            img_E = Image.fromarray(out_masks[f])\n","            img_E.putpalette(palette)\n","            img_E.save(os.path.join(this_out_path, '{:05d}.png'.format(f)))\n","\n","\n","        # Adapted from the github of STM\n","        # https://github.com/seoungwugoh/STM/blob/master/eval_DAVIS.py\n","        if VIZ:\n","          from helpers import overlay_davis\n","          # visualize results\n","          viz_path = os.path.join(output_path, name) \n","          if not os.path.exists(viz_path):\n","              os.makedirs(viz_path)\n","\n","          for f in range(out_masks.shape[0]):\n","              im = rgb[0,f]  \n","              im = im * std + mean\n","              pF = (im.permute(1,2,0).cpu().numpy() * 255.).astype(np.uint8)\n","              pE = out_masks[f]\n","              canvas = overlay_davis(pF, pE, palette)\n","              canvas = Image.fromarray(canvas)\n","              canvas.save(os.path.join(viz_path, 'f{}.jpg'.format(f)))\n","\n","          vid_path = os.path.join(output_path, '{}.mp4'.format(name))\n","          frame_path = os.path.join(output_path, name, 'f%d.jpg')\n","          os.system('ffmpeg -framerate 10 -i {} {} -vcodec libx264 -crf 10  -pix_fmt yuv420p  -nostats -loglevel 0 -y'.format(frame_path, vid_path))\n","\n","\n","        del rgb\n","        del msk_p\n","        del processor\n","\n","print('Total processing time: ', total_process_time)\n","print('Total processed frames: ', total_frames)\n","print('FPS: ', total_frames / total_process_time)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jE6cNsejFy16","outputId":"8117e5b9-53d2-4e00-cc46-36b76f6d3427"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Total processing time:  117.11059904098511\n","Total processed frames:  1999\n","FPS:  17.06933459797615\n"]}]},{"cell_type":"markdown","source":["#implementing Detectron2\n"],"metadata":{"id":"vzzIBYDVwKnD"}},{"cell_type":"code","source":["%cd /content/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SQ5bx8a0xI4_","outputId":"da69071e-5389-4fe4-b450-4775da57f448"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content\n"]}]},{"cell_type":"code","source":["# install dependencies: \n","!pip install pyyaml==5.1\n","# clone the repo in order to access pre-defined configs in PointRend project\n","!git clone --branch v0.6 https://github.com/facebookresearch/detectron2.git detectron2_repo\n","# install detectron2 from source\n","!pip install -e detectron2_repo"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"bwDbzeur6TAh","outputId":"5f434f4e-6063-4061-aea8-0698da766800"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pyyaml==5.1\n","  Downloading PyYAML-5.1.tar.gz (274 kB)\n","\u001b[?25l\r\u001b[K     |█▏                              | 10 kB 37.3 MB/s eta 0:00:01\r\u001b[K     |██▍                             | 20 kB 39.1 MB/s eta 0:00:01\r\u001b[K     |███▋                            | 30 kB 44.6 MB/s eta 0:00:01\r\u001b[K     |████▉                           | 40 kB 28.9 MB/s eta 0:00:01\r\u001b[K     |██████                          | 51 kB 17.9 MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 61 kB 18.7 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 71 kB 14.4 MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 81 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 92 kB 16.7 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 102 kB 14.7 MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 112 kB 14.7 MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 122 kB 14.7 MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 133 kB 14.7 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 143 kB 14.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 153 kB 14.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 163 kB 14.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 174 kB 14.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 184 kB 14.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 194 kB 14.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 204 kB 14.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 215 kB 14.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 225 kB 14.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 235 kB 14.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 245 kB 14.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 256 kB 14.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 266 kB 14.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 274 kB 14.7 MB/s \n","\u001b[?25hBuilding wheels for collected packages: pyyaml\n","  Building wheel for pyyaml (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyyaml: filename=PyYAML-5.1-cp37-cp37m-linux_x86_64.whl size=44092 sha256=25e3a8ef57efae14046b2eca43f6fb18ff563788b629809452a101db247eb7da\n","  Stored in directory: /root/.cache/pip/wheels/77/f5/10/d00a2bd30928b972790053b5de0c703ca87324f3fead0f2fd9\n","Successfully built pyyaml\n","Installing collected packages: pyyaml\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed pyyaml-5.1\n","Cloning into 'detectron2_repo'...\n","remote: Enumerating objects: 13863, done.\u001b[K\n","remote: Total 13863 (delta 0), reused 0 (delta 0), pack-reused 13863\u001b[K\n","Receiving objects: 100% (13863/13863), 5.68 MiB | 24.97 MiB/s, done.\n","Resolving deltas: 100% (10015/10015), done.\n","Note: checking out 'd1e04565d3bec8719335b88be9e9b961bf3ec464'.\n","\n","You are in 'detached HEAD' state. You can look around, make experimental\n","changes and commit them, and you can discard any commits you make in this\n","state without impacting any branches by performing another checkout.\n","\n","If you want to create a new branch to retain commits you create, you may\n","do so (now or later) by using -b with the checkout command again. Example:\n","\n","  git checkout -b <new-branch-name>\n","\n","Obtaining file:///content/detectron2_repo\n","Requirement already satisfied: Pillow>=7.1 in /usr/local/lib/python3.7/dist-packages (from detectron2==0.6) (7.1.2)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from detectron2==0.6) (3.2.2)\n","Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from detectron2==0.6) (2.0.4)\n","Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.7/dist-packages (from detectron2==0.6) (1.1.0)\n","Collecting yacs>=0.1.8\n","  Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n","Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from detectron2==0.6) (0.8.9)\n","Requirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from detectron2==0.6) (1.3.0)\n","Requirement already satisfied: tqdm>4.29.0 in /usr/local/lib/python3.7/dist-packages (from detectron2==0.6) (4.62.3)\n","Requirement already satisfied: tensorboard in /usr/local/lib/python3.7/dist-packages (from detectron2==0.6) (2.7.0)\n","Collecting fvcore<0.1.6,>=0.1.5\n","  Downloading fvcore-0.1.5.post20220119.tar.gz (55 kB)\n","\u001b[K     |████████████████████████████████| 55 kB 3.4 MB/s \n","\u001b[?25hCollecting iopath<0.1.10,>=0.1.7\n","  Downloading iopath-0.1.9-py3-none-any.whl (27 kB)\n","Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from detectron2==0.6) (0.16.0)\n","Requirement already satisfied: pydot in /usr/local/lib/python3.7/dist-packages (from detectron2==0.6) (1.3.0)\n","Collecting omegaconf>=2.1\n","  Downloading omegaconf-2.1.1-py3-none-any.whl (74 kB)\n","\u001b[K     |████████████████████████████████| 74 kB 3.7 MB/s \n","\u001b[?25hCollecting hydra-core>=1.1\n","  Downloading hydra_core-1.1.1-py3-none-any.whl (145 kB)\n","\u001b[K     |████████████████████████████████| 145 kB 71.8 MB/s \n","\u001b[?25hCollecting black==21.4b2\n","  Downloading black-21.4b2-py3-none-any.whl (130 kB)\n","\u001b[K     |████████████████████████████████| 130 kB 67.5 MB/s \n","\u001b[?25hRequirement already satisfied: appdirs in /usr/local/lib/python3.7/dist-packages (from black==21.4b2->detectron2==0.6) (1.4.4)\n","Collecting mypy-extensions>=0.4.3\n","  Downloading mypy_extensions-0.4.3-py2.py3-none-any.whl (4.5 kB)\n","Collecting typed-ast>=1.4.2\n","  Downloading typed_ast-1.5.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (843 kB)\n","\u001b[K     |████████████████████████████████| 843 kB 71.4 MB/s \n","\u001b[?25hRequirement already satisfied: toml>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from black==21.4b2->detectron2==0.6) (0.10.2)\n","Requirement already satisfied: click>=7.1.2 in /usr/local/lib/python3.7/dist-packages (from black==21.4b2->detectron2==0.6) (7.1.2)\n","Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from black==21.4b2->detectron2==0.6) (3.10.0.2)\n","Collecting pathspec<1,>=0.8.1\n","  Downloading pathspec-0.9.0-py2.py3-none-any.whl (31 kB)\n","Collecting regex>=2020.1.8\n","  Downloading regex-2022.1.18-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (748 kB)\n","\u001b[K     |████████████████████████████████| 748 kB 64.9 MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fvcore<0.1.6,>=0.1.5->detectron2==0.6) (1.19.5)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from fvcore<0.1.6,>=0.1.5->detectron2==0.6) (5.1)\n","Collecting antlr4-python3-runtime==4.8\n","  Downloading antlr4-python3-runtime-4.8.tar.gz (112 kB)\n","\u001b[K     |████████████████████████████████| 112 kB 71.5 MB/s \n","\u001b[?25hRequirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from hydra-core>=1.1->detectron2==0.6) (5.4.0)\n","Collecting portalocker\n","  Downloading portalocker-2.3.2-py2.py3-none-any.whl (15 kB)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->detectron2==0.6) (2.8.2)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->detectron2==0.6) (3.0.6)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->detectron2==0.6) (1.3.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->detectron2==0.6) (0.11.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->detectron2==0.6) (1.15.0)\n","Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources->hydra-core>=1.1->detectron2==0.6) (3.7.0)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2==0.6) (2.23.0)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2==0.6) (0.12.0)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2==0.6) (0.6.1)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2==0.6) (1.0.1)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2==0.6) (0.37.1)\n","Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2==0.6) (3.17.3)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2==0.6) (0.4.6)\n","Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2==0.6) (1.43.0)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2==0.6) (1.8.1)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2==0.6) (57.4.0)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2==0.6) (1.35.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2==0.6) (3.3.6)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->detectron2==0.6) (0.2.8)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->detectron2==0.6) (4.8)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->detectron2==0.6) (4.2.4)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->detectron2==0.6) (1.3.0)\n","Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard->detectron2==0.6) (4.10.0)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->detectron2==0.6) (0.4.8)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.6) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.6) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.6) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.6) (2021.10.8)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->detectron2==0.6) (3.1.1)\n","Building wheels for collected packages: fvcore, antlr4-python3-runtime\n","  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for fvcore: filename=fvcore-0.1.5.post20220119-py3-none-any.whl size=65267 sha256=52a68112e13a73daff37613e3af8f939b9d1cb0dab6b3bd240a3717f23bfe35b\n","  Stored in directory: /root/.cache/pip/wheels/f3/b8/eb/61ed840f80d7198725bc061872b6019a7b3e9db4dbadf68083\n","  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-py3-none-any.whl size=141230 sha256=84f2dd1241aa5c63540e2573e34a9136c0145f8cc59f81eeb9740d559183a27c\n","  Stored in directory: /root/.cache/pip/wheels/ca/33/b7/336836125fc9bb4ceaa4376d8abca10ca8bc84ddc824baea6c\n","Successfully built fvcore antlr4-python3-runtime\n","Installing collected packages: portalocker, antlr4-python3-runtime, yacs, typed-ast, regex, pathspec, omegaconf, mypy-extensions, iopath, hydra-core, fvcore, black, detectron2\n","  Attempting uninstall: regex\n","    Found existing installation: regex 2019.12.20\n","    Uninstalling regex-2019.12.20:\n","      Successfully uninstalled regex-2019.12.20\n","  Running setup.py develop for detectron2\n","Successfully installed antlr4-python3-runtime-4.8 black-21.4b2 detectron2-0.6 fvcore-0.1.5.post20220119 hydra-core-1.1.1 iopath-0.1.9 mypy-extensions-0.4.3 omegaconf-2.1.1 pathspec-0.9.0 portalocker-2.3.2 regex-2022.1.18 typed-ast-1.5.1 yacs-0.1.8\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["pydevd_plugins"]}}},"metadata":{}}]},{"cell_type":"code","source":["%cd /content/detectron2_repo/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CWYVHAiN4SVR","outputId":"64880a56-edfb-415b-cc6b-c0e3097c30e5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/detectron2_repo\n"]}]},{"cell_type":"code","source":["# You may need to restart your runtime prior to this, to let your installation take effect\n","# Some basic setup:\n","# Setup detectron2 logger\n","import detectron2\n","from detectron2.utils.logger import setup_logger\n","setup_logger()\n","\n","# import some common libraries\n","import numpy as np\n","import cv2\n","import torch\n","from google.colab.patches import cv2_imshow\n","\n","# import some common detectron2 utilities\n","from detectron2 import model_zoo\n","from detectron2.engine import DefaultPredictor\n","from detectron2.config import get_cfg\n","from detectron2.utils.visualizer import Visualizer, ColorMode\n","from detectron2.data import MetadataCatalog\n","coco_metadata = MetadataCatalog.get(\"coco_2017_val\")\n","\n","# import PointRend project\n","from detectron2.projects import point_rend"],"metadata":{"id":"lcQSUxNFbAB4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Evaluation on STCN model using pretrained PointRend as 1st frame segmentation"],"metadata":{"id":"N1mQvms4obSm"}},{"cell_type":"markdown","source":["## Calling pretrained model"],"metadata":{"id":"dZy6tLi4N2jg"}},{"cell_type":"code","source":["cfg = get_cfg()\n","point_rend.add_pointrend_config(cfg)\n","cfg.merge_from_file(\"./projects/PointRend/configs/InstanceSegmentation/pointrend_rcnn_R_50_FPN_3x_coco.yaml\") \n","cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.01  # set threshold for this model\n","cfg.MODEL.WEIGHTS = \"detectron2://PointRend/InstanceSegmentation/pointrend_rcnn_R_50_FPN_3x_coco/164955410/model_final_edd263.pkl\"\n","\n","predictor = DefaultPredictor(cfg)"],"metadata":{"id":"o3sa9M9K08w_","colab":{"base_uri":"https://localhost:8080/"},"outputId":"0f44c209-abf4-4670-ba65-2590730d82ac"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["model_final_edd263.pkl: 241MB [00:11, 20.1MB/s]                           \n"]},{"output_type":"stream","name":"stdout","text":["\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/22 20:13:50 d2.projects.point_rend.mask_head]: \u001b[0mWeight format of PointRend models have changed! Applying automatic conversion now ...\n"]}]},{"cell_type":"code","source":["### Hyper parameters ###\n","\n","VIZ = False ## Option to generate video with mask (add more processing time)\n","\n","mean = torch.tensor([0.485, 0.456, 0.406])[:, None, None]\n","std = torch.tensor([0.229, 0.224, 0.225])[:, None, None]\n","\n","output_path = '/content/drive/MyDrive/STCN/experiment/Davis2017/Yujin/val_pointrend_22jan/'"],"metadata":{"id":"KAtTwOq0N-de"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["total_process_time = 0\n","total_frames = 0\n","\n","for data in test_loader:\n","    with torch.cuda.amp.autocast(enabled=args.amp):\n","        rgb = data['rgb']\n","        msk = data['gt'][0].cuda() # original annotation\n","        info = data['info']\n","        name = info['name'][0]\n","        k = len(info['labels'][0])\n","        size = info['size_480p']\n","\n","        #############################################################################\n","        ######################### SEGMENTATION ALGORTIMH ############################\n","        #############################################################################\n","        msk_p = algo_pointrend(predictor,msk,rgb,std,mean)\n","        #############################################################################\n","\n","        torch.cuda.synchronize()\n","        process_begin = time.time()\n","\n","        processor = InferenceCore(prop_model, rgb, k, top_k=top_k, \n","                        mem_every=args.mem_every, include_last=args.include_last)\n","        processor.interact(msk_p, 0, rgb.shape[1]) #msk_p changed here\n","\n","        # Do unpad -> upsample to original size \n","        out_masks = torch.zeros((processor.t, 1, *size), dtype=torch.uint8, device='cuda')\n","        for ti in range(processor.t):\n","            prob = unpad(processor.prob[:,ti], processor.pad)\n","            prob = F.interpolate(prob, size, mode='bilinear', align_corners=False)\n","            out_masks[ti] = torch.argmax(prob, dim=0)\n","        \n","        out_masks = (out_masks.detach().cpu().numpy()[:,0]).astype(np.uint8)\n","\n","        torch.cuda.synchronize()\n","        total_process_time += time.time() - process_begin\n","        total_frames += out_masks.shape[0]\n","\n","        # Save the results\n","        this_out_path = path.join(out_path, name)\n","        os.makedirs(this_out_path, exist_ok=True)\n","        for f in range(out_masks.shape[0]):\n","            img_E = Image.fromarray(out_masks[f])\n","            img_E.putpalette(palette)\n","            img_E.save(os.path.join(this_out_path, '{:05d}.png'.format(f)))\n","\n","\n","        # Adapted from the github of STM\n","        # https://github.com/seoungwugoh/STM/blob/master/eval_DAVIS.py\n","        if VIZ:\n","          from helpers import overlay_davis\n","          # visualize results\n","          viz_path = os.path.join(output_path, name) \n","          if not os.path.exists(viz_path):\n","              os.makedirs(viz_path)\n","\n","          for f in range(out_masks.shape[0]):\n","              im = rgb[0,f]  \n","              im = im * std + mean\n","              pF = (im.permute(1,2,0).cpu().numpy() * 255.).astype(np.uint8)\n","              pE = out_masks[f]\n","              canvas = overlay_davis(pF, pE, palette)\n","              canvas = Image.fromarray(canvas)\n","              canvas.save(os.path.join(viz_path, 'f{}.jpg'.format(f)))\n","\n","          vid_path = os.path.join(output_path, '{}.mp4'.format(name))\n","          frame_path = os.path.join(output_path, name, 'f%d.jpg')\n","          os.system('ffmpeg -framerate 10 -i {} {} -vcodec libx264 -crf 10  -pix_fmt yuv420p  -nostats -loglevel 0 -y'.format(frame_path, vid_path))\n","\n","\n","        del rgb\n","        del msk_p\n","        del processor\n","\n","print('Total processing time: ', total_process_time)\n","print('Total processed frames: ', total_frames)\n","print('FPS: ', total_frames / total_process_time)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nMvYWFwsF_6F","outputId":"2d06c4d2-ac59-47cd-9bcb-80f1754931c3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/content/detectron2_repo/detectron2/structures/image_list.py:88: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n","  max_size = (max_size + (stride - 1)) // stride * stride\n","/content/detectron2_repo/projects/PointRend/point_rend/point_features.py:142: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n","  point_coords[:, :, 1] = h_step / 2.0 + (point_indices // W).to(torch.float) * h_step\n"]},{"output_type":"stream","name":"stdout","text":["Total processing time:  134.72681999206543\n","Total processed frames:  1999\n","FPS:  14.837431775779526\n"]}]}]}