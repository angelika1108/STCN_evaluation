{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Task_3_Make_Masks_LOST_and_CRF.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["### **Applying LOST to segment the first video frame**\n","\n","For implementing LOST: https://arxiv.org/pdf/2109.14279.pdf, we adapted the code of the paper, which is given on the following GitHub page: https://github.com/valeoai/LOST.\n","\n","First we are going to apply LOST on all first frames of the Davis2017 dataset.\n","\n","Then, to improve the masks, we are going to apply the CRF processing step from the paper: https://arxiv.org/pdf/1210.5644.pdf, by adapting the following GitHub code: https://github.com/lucasb-eyer/pydensecrf."],"metadata":{"id":"_qnOjSdyWRtM"}},{"cell_type":"code","source":["!git clone https://github.com/valeoai/LOST"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GCf670g_pvgA","outputId":"34999cf9-883b-44e9-f852-3be7fd90535b","executionInfo":{"status":"ok","timestamp":1642352946147,"user_tz":-60,"elapsed":238,"user":{"displayName":"Angelika Andó","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghe4AyiaNK3SDGFPjbSDzvTKiY8vU9Pw_f0Mo_6=s64","userId":"02461689410513371892"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["fatal: destination path 'LOST' already exists and is not an empty directory.\n"]}]},{"cell_type":"code","source":["!git clone https://github.com/facebookresearch/dino.git"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kR_L4grNt7dH","outputId":"2980173c-6c42-4407-821c-0d15b53cc5ca","executionInfo":{"status":"ok","timestamp":1642352946939,"user_tz":-60,"elapsed":8,"user":{"displayName":"Angelika Andó","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghe4AyiaNK3SDGFPjbSDzvTKiY8vU9Pw_f0Mo_6=s64","userId":"02461689410513371892"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["fatal: destination path 'dino' already exists and is not an empty directory.\n"]}]},{"cell_type":"code","source":["!pip install git+https://github.com/lucasb-eyer/pydensecrf.git"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VYC87wq9tA6K","executionInfo":{"status":"ok","timestamp":1642352962992,"user_tz":-60,"elapsed":16056,"user":{"displayName":"Angelika Andó","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghe4AyiaNK3SDGFPjbSDzvTKiY8vU9Pw_f0Mo_6=s64","userId":"02461689410513371892"}},"outputId":"7fe5584c-1156-4980-aa5e-26eefb860cae"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting git+https://github.com/lucasb-eyer/pydensecrf.git\n","  Cloning https://github.com/lucasb-eyer/pydensecrf.git to /tmp/pip-req-build-idr_3azz\n","  Running command git clone -q https://github.com/lucasb-eyer/pydensecrf.git /tmp/pip-req-build-idr_3azz\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n"]}]},{"cell_type":"code","source":["# Access to dataset through Drive\n","import os\n","from google.colab import drive\n","drive.mount('/content/drive/', force_remount=True)\n","os.chdir('/content/drive/My Drive/')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bNy6a1GK_2xY","outputId":"9a95407e-2afa-48d8-e9bd-b39398d1e651","executionInfo":{"status":"ok","timestamp":1642352967162,"user_tz":-60,"elapsed":4178,"user":{"displayName":"Angelika Andó","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghe4AyiaNK3SDGFPjbSDzvTKiY8vU9Pw_f0Mo_6=s64","userId":"02461689410513371892"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n"]}]},{"cell_type":"code","source":["import sys\n","import numpy as np\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","import torch"],"metadata":{"id":"SMVrm78onSJD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sys.path.insert(0, '/content/LOST')\n","sys.path.insert(1, '/content/dino')\n","sys.path.insert(2, '/content/drive/MyDrive/STCN/DAVIS/2017/trainval/JPEGImages/480p')\n","sys.path.insert(3, '/content/drive/MyDrive/STCN/experiment/LOST')\n","sys.path.insert(4, '/content/drive/MyDrive/STCN/DAVIS/2017/trainval/Annotations/480p')\n","sys.path.insert(5, '/content/drive/MyDrive/STCN/STCN-LOST-CRF/Annotations')"],"metadata":{"id":"Oyv7lPHi0CNg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from object_discovery import patch_scoring\n","from skimage.transform import resize\n","import scipy\n","import itertools\n","import glob\n","import pydensecrf.densecrf as dcrf"],"metadata":{"id":"nfPlKcOSK6cH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Model\n","k_patches = 100    # Number of patches with the lowest degree considered\n","patch_size = 16\n","from networks import get_model\n","device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n","model = get_model(\"vit_base\", patch_size = patch_size, resnet_dilate=0, device=device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jfJlqQU60AZZ","executionInfo":{"status":"ok","timestamp":1642352969679,"user_tz":-60,"elapsed":2521,"user":{"displayName":"Angelika Andó","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghe4AyiaNK3SDGFPjbSDzvTKiY8vU9Pw_f0Mo_6=s64","userId":"02461689410513371892"}},"outputId":"adb99929-3467-45d7-df55-dea5a9cef491"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Since no pretrained weights have been provided, we load the reference pretrained DINO weights.\n","Pretrained weights found at dino_vitbase16_pretrain/dino_vitbase16_pretrain.pth and loaded with msg: <All keys matched successfully>\n"]}]},{"cell_type":"code","source":["# Image transformation\n","from torchvision import transforms as pth_transforms\n","transform = pth_transforms.Compose(\n","    [\n","        pth_transforms.ToTensor(),\n","        pth_transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n","    ]\n",")"],"metadata":{"id":"Uhj9Jyfo0GK9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Slightly modified LOST\n","# Instead of returning the bounding box predictions, it returns the mask\n","def modified_lost(feats, dims, scales, init_image_size, k_patches=100):\n","    \"\"\"\n","    Inputs\n","        feats: the pixel/patche features of an image\n","        dims: dimension of the map from which the features are used\n","        scales: from image to map scale\n","        init_image_size: size of the image\n","        k_patches: number of k patches retrieved that are compared to the seed at seed expansion\n","    Outputs\n","        M: mask\n","        seed: selected patch corresponding to an object\n","    \"\"\"\n","    # Compute the similarity\n","    A = (feats @ feats.transpose(1, 2)).squeeze()\n","\n","    # Compute the inverse degree centrality measure per patch\n","    sorted_patches, scores = patch_scoring(A)\n","\n","    # Select the initial seed\n","    seed = sorted_patches[0]\n","\n","    # Seed expansion\n","    potentials = sorted_patches[:k_patches]\n","    similars = potentials[A[seed, potentials] > 0.0]\n","    M = torch.sum(A[similars, :], dim=0)\n","\n","    return M, seed"],"metadata":{"id":"h9yeyrHaFRKZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Video folder names\n","for folder in os.walk(sys.path[2], topdown=True):\n","    video_names = folder[1]\n","    break\n","\n","print('Video folder names: ', video_names)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZCuLj7hS9_-E","executionInfo":{"status":"ok","timestamp":1642352969681,"user_tz":-60,"elapsed":7,"user":{"displayName":"Angelika Andó","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghe4AyiaNK3SDGFPjbSDzvTKiY8vU9Pw_f0Mo_6=s64","userId":"02461689410513371892"}},"outputId":"5fa0b8b6-3657-41b2-fee0-663f56a97fec"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Video folder names:  ['bear', 'bike-packing', 'blackswan', 'bmx-bumps', 'bmx-trees', 'boat', 'boxing-fisheye', 'breakdance', 'breakdance-flare', 'bus', 'camel', 'car-roundabout', 'car-shadow', 'car-turn', 'cat-girl', 'classic-car', 'color-run', 'cows', 'crossing', 'dance-jump', 'dance-twirl', 'dancing', 'disc-jockey', 'dog', 'dog-agility', 'dog-gooses', 'dogs-jump', 'dogs-scale', 'drift-chicane', 'drift-straight', 'drift-turn', 'drone', 'elephant', 'flamingo', 'goat', 'gold-fish', 'hike', 'hockey', 'horsejump-high', 'horsejump-low', 'india', 'judo', 'kid-football', 'kite-surf', 'kite-walk', 'koala', 'lab-coat', 'lady-running', 'libby', 'lindy-hop', 'loading', 'longboard', 'lucia', 'mallard-fly', 'mallard-water', 'mbike-trick', 'miami-surf', 'motocross-bumps', 'motocross-jump', 'motorbike', 'night-race', 'paragliding', 'paragliding-launch', 'parkour', 'pigs', 'planes-water', 'rallye', 'rhino', 'rollerblade', 'schoolgirls', 'scooter-black', 'scooter-board', 'scooter-gray', 'sheep', 'shooting', 'skate-park', 'snowboard', 'soapbox', 'soccerball', 'stroller', 'stunt', 'surf', 'swing', 'tennis', 'tractor-sand', 'train', 'tuk-tuk', 'upside-down', 'varanus-cage', 'walking']\n"]}]},{"cell_type":"code","source":["for video_name in video_names:\n","    image_path = os.path.join(sys.path[2], video_name, '00000.jpg')  # First frames in the folders\n","    image = Image.open(image_path)\n","    image = np.array(image)\n","    image_size = image.shape                    # array (image_height, image_width, 3)\n","    image_height, image_width, _ = image_size\n","\n","    if not os.path.isdir(os.path.join(sys.path[3], video_name)):\n","        os.makedirs(os.path.join(sys.path[3], video_name))\n","\n","    # Apply LOST\n","    img = image\n","    img = transform(img)\n","    init_image_size = img.shape    # tensor [3, image_height, image_width]\n","    im_name = '00000'\n","\n","    # Padding the image with zeros to fit multiple of patch-size\n","    size_im = (\n","        img.shape[0],\n","        int(np.ceil(img.shape[1] / patch_size) * patch_size),\n","        int(np.ceil(img.shape[2] / patch_size) * patch_size),\n","    )\n","    paded = torch.zeros(size_im)\n","    paded[:, : img.shape[1], : img.shape[2]] = img\n","    img = paded\n","    # img = img.cuda(non_blocking=True)    # Move to gpu\n","\n","    # Size for transformers\n","    w_featmap = img.shape[-2] // patch_size\n","    h_featmap = img.shape[-1] // patch_size\n","\n","    which_features = \"k\"    # possible choices : \"q\", \"k\", \"v\"\n","\n","    with torch.no_grad():\n","        # Store the outputs of qkv layer from the last attention layer\n","        feat_out = {}\n","        def hook_fn_forward_qkv(module, input, output):\n","            feat_out[\"qkv\"] = output\n","\n","        model._modules[\"blocks\"][-1]._modules[\"attn\"]._modules[\"qkv\"].register_forward_hook(hook_fn_forward_qkv)\n","\n","        # Forward pass in the model\n","        attentions = model.get_last_selfattention(img[None, :, :, :])\n","\n","        # Scaling factor\n","        scales = [patch_size, patch_size]\n","\n","        # Dimensions\n","        nb_im = attentions.shape[0]  # Batch size\n","        nh = attentions.shape[1]  # Number of heads\n","        nb_tokens = attentions.shape[2]  # Number of tokens\n","\n","        # Extract the qkv features of the last attention layer\n","        qkv = (\n","            feat_out[\"qkv\"]\n","            .reshape(nb_im, nb_tokens, 3, nh, -1 // nh)\n","            .permute(2, 0, 3, 1, 4)\n","        )\n","        q, k, v = qkv[0], qkv[1], qkv[2]\n","        k = k.transpose(1, 2).reshape(nb_im, nb_tokens, -1)\n","        q = q.transpose(1, 2).reshape(nb_im, nb_tokens, -1)\n","        v = v.transpose(1, 2).reshape(nb_im, nb_tokens, -1)\n","\n","        # Modality selection\n","        if which_features == \"k\":     # keys of the patches\n","            feats = k[:, 1:, :]       # tensor [1, 1620, 768]\n","        elif which_features == \"q\":\n","            feats = q[:, 1:, :]\n","        elif which_features == \"v\":\n","            feats = v[:, 1:, :]\n","\n","    # Apply modified LOST\n","    M, seed = modified_lost(\n","        feats,\n","        [w_featmap, h_featmap],\n","        scales,\n","        init_image_size,\n","        k_patches=k_patches,\n","    )\n","\n","    # Mask extraction\n","\n","    # Reshape M\n","    correl = M.reshape(w_featmap, h_featmap).float()    # tensor [w_featmap, h_featmap]\n","\n","    # Compute connected components\n","    labeled_array, num_features = scipy.ndimage.label(correl.cpu().numpy() > 0.0)\n","\n","    # Find connected component corresponding to the initial seed\n","    cc = labeled_array[np.unravel_index(seed.cpu().numpy(), (w_featmap, h_featmap))]\n","\n","    # Compute mask containing the seed\n","    mask = (labeled_array == cc)            # array (w_featmap, h_featmap)\n","\n","    # Size of the image not considering the channels\n","    image_size_2d = image_size[:2]\n","\n","    # Resize the mask to the size of the image\n","    resized_mask = resize(mask, image_size_2d)    # Mask corresponding to the connected component containing the seed\n","\n","    # Colour the mask red\n","    black_image = 0 * image                 # array (image_height, image_width, 3)\n","    black_image[resized_mask == 1, 0] = 255\n","    red_mask = black_image\n","    red_mask = Image.fromarray(red_mask, 'RGB')\n","    red_mask.save(os.path.join(sys.path[3], video_name, 'red_mask.png'))  # Save mask obtained by LOST\n","\n","    # Apply CRF processing step\n","    # Unary potential\n","    # U[0, :, :] : channel of the background\n","    # U[1, :, :] : channel of the object\n","    U = np.ones((2, resized_mask.shape[0], resized_mask.shape[1]), dtype=np.float32)  # array (2, image_height, image_width)\n","    U[0, :, :] = 10\n","    U[1, resized_mask==1] = 100\n","    U = U / U.sum(0, keepdims=True)    # Probability of the classes at different positions\n","\n","    d = dcrf.DenseCRF2D(image_width, image_height, 2)  # Width, height, nlabels\n","\n","    U = - np.log(U)    # Minus log probability\n","    U = U.reshape((2,-1)) # Needs to be flat\n","    d.setUnaryEnergy(U)\n","\n","    # This adds the color-independent term, features are the locations only\n","    d.addPairwiseGaussian(sxy=(3, 3), compat=3, kernel=dcrf.DIAG_KERNEL,\n","                            normalization=dcrf.NORMALIZE_SYMMETRIC)\n","\n","    # This adds the color-dependent term, i.e. features are (x,y,r,g,b)\n","    d.addPairwiseBilateral(sxy=(40, 40), srgb=(13, 13, 13), rgbim=image,\n","                            compat=10,\n","                            kernel=dcrf.DIAG_KERNEL,\n","                            normalization=dcrf.NORMALIZE_SYMMETRIC)\n","\n","    Q = d.inference(10)\n","\n","    # Find out the most probable class for each pixel\n","    MAP = np.argmax(Q, axis=0)\n","\n","    reshaped_MAP = MAP.reshape((image_height, image_width))\n","\n","    # Colour the new mask red\n","    black_mask = 0 * image                   # array (image_height, image_width, 3)\n","    black_mask[reshaped_MAP == 1, 0] = 255\n","    new_mask = black_mask\n","    new_mask = Image.fromarray(new_mask, 'RGB')\n","    new_mask.save(os.path.join(sys.path[3], video_name, 'new_mask.png'))  # Save mask obtained by LOST+CRF\n","\n","    if not os.path.isdir(os.path.join(sys.path[5], video_name)):\n","        os.makedirs(os.path.join(sys.path[5], video_name))\n","    new_mask.save(os.path.join(sys.path[5], video_name, '00000.png'))  # Save mask for applying STCN later\n","\n","    # Save original mask\n","    original_mask_path = os.path.join(sys.path[4], video_name, '00000.png')\n","    original_mask = Image.open(original_mask_path)\n","    original_mask.save(os.path.join(sys.path[3], video_name, 'original_mask.png'))\n","\n","    # Save original image\n","    image = Image.fromarray(image, 'RGB')\n","    image.save(os.path.join(sys.path[3], video_name, 'image.jpg'))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M4Se5qcNoCb1","outputId":"ce5024ac-afbc-4673-88db-0129b46cff0f","executionInfo":{"status":"ok","timestamp":1642353830140,"user_tz":-60,"elapsed":860465,"user":{"displayName":"Angelika Andó","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghe4AyiaNK3SDGFPjbSDzvTKiY8vU9Pw_f0Mo_6=s64","userId":"02461689410513371892"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3635: UserWarning: Default upsampling behavior when mode=bicubic is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n","  \"See the documentation of nn.Upsample for details.\".format(mode)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"_nnPA7IfccGG"},"execution_count":null,"outputs":[]}]}